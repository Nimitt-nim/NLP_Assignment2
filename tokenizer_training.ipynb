{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AH75GkdZbvR7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "import argparse\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oAvIF1k3cAtj"
      },
      "outputs": [],
      "source": [
        "def train_tokenizer(data_list, vocab_size=32768, model_name=\"test\"):\n",
        "\n",
        "    ## Change bos & eos\n",
        "    bos_tok = \"<sos>\"\n",
        "    eos_tok = \"<end_of_sen>\"\n",
        "\n",
        "    ## Add basic characters to this below list, including numbers & special language characters.\n",
        "    special_char = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "\n",
        "\n",
        "    tokenizer = SentencePieceBPETokenizer()\n",
        "\n",
        "    tokenizer.train_from_iterator(\n",
        "        data_list,\n",
        "        vocab_size = vocab_size,\n",
        "        min_frequency = 5,\n",
        "        special_tokens = [\"<pad>\", \"<unk>\", bos_tok, eos_tok, \"<user>\", \"<assistant>\"] + special_char,\n",
        "        show_progress = True,\n",
        "    )\n",
        "\n",
        "    ## Don't forget to add special tokens.\n",
        "    transformer_tokenizer = PreTrainedTokenizerFast(\n",
        "        tokenizer_object=tokenizer,\n",
        "        bos_token = bos_tok,\n",
        "        eos_token = eos_tok,\n",
        "        unk_token = \"<unk>\",\n",
        "        pad_token = \"<pad>\",\n",
        "        mask_token = \"<mask>\",\n",
        "        padding_side = \"left\",\n",
        "        truncation_side = \"right\",\n",
        "        additional_special_tokens = [\"<user>\", \"<assistant>\"],\n",
        "        clean_up_tokenization_spaces = False,\n",
        "    )\n",
        "\n",
        "    transformer_tokenizer.save_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "91P3QAjXfvv_"
      },
      "outputs": [],
      "source": [
        "### Importing Data\n",
        "df = pd.read_csv(\"tokenizer_train.csv\")\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTS3LE5sgar4",
        "outputId": "497b76df-302c-46b0-a8ca-dee2eb393cfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "432647"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df[\"Input\"].to_list())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vUkD5EYQb_-T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Executing Training Function to Train tokenizer\n",
        "train_tokenizer(df[\"Input\"].to_list())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
